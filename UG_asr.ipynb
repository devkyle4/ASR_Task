{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZSfPw6tPnTp",
        "outputId": "fa3c8bfa-eb78-43e8-ed6f-9d3a3c579cd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dropbox\n",
            "  Downloading dropbox-11.36.2-py3-none-any.whl (594 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.0/594.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (2.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from dropbox) (1.16.0)\n",
            "Collecting stone>=2 (from dropbox)\n",
            "  Downloading stone-3.3.3-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.6/158.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2024.2.2)\n",
            "Collecting ply>=3.4 (from stone>=2->dropbox)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ply, stone, dropbox\n",
            "Successfully installed dropbox-11.36.2 ply-3.11 stone-3.3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install dropbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JOOu2lPEpclq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import torch\n",
        "import dropbox\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as T\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "lY9A9eK3gfl4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uDTGCd0_A4kv"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GMNswJeBagWL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "dbx_token = userdata.get('dbx_token')\n",
        "dbx = dropbox.Dropbox(dbx_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YrskzXRifD3G"
      },
      "outputs": [],
      "source": [
        "# STREAM DATA FROM DROPBOX\n",
        "\n",
        "def stream_data_from_dropbox(path):\n",
        "  _, res = dbx.files_download(path)\n",
        "  data = io.BytesIO(res.content)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SGcyb6oviA_q"
      },
      "outputs": [],
      "source": [
        "audio_transcript_path=\"/Akan/selected transcribed audios/selected transcribed audios.xlsx\"\n",
        "audio_transcript_data = stream_data_from_dropbox(audio_transcript_path)\n",
        "transcribed_audios_df = pd.read_excel(audio_transcript_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "  return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "def levenshtein_distance(ref, hyp):\n",
        "  m = len(ref)\n",
        "  n = len(hyp)\n",
        "\n",
        "  # Special Case\n",
        "  if ref == hyp:\n",
        "    return 0\n",
        "  if m == 0:\n",
        "    return n\n",
        "  if n == 0:\n",
        "    return m\n",
        "\n",
        "  if m<n:\n",
        "    ref, hyp = hyp, ref\n",
        "    m, n = n, m\n",
        "\n",
        "\n",
        "  distance = np.zeros((2, n+1), dtype=np.int32)\n",
        "\n",
        "  # initialize distance matrix\n",
        "  for j in range(0,n + 1):\n",
        "      distance[0][j] = j\n",
        "\n",
        "  # calculate levenshtein distance\n",
        "  for i in range(1, m + 1):\n",
        "      prev_row_idx = (i - 1) % 2\n",
        "      cur_row_idx = i % 2\n",
        "      distance[cur_row_idx][0] = i\n",
        "      for j in range(1, n + 1):\n",
        "          if ref[i - 1] == hyp[j - 1]:\n",
        "              distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "          else:\n",
        "              s_num = distance[prev_row_idx][j - 1] + 1\n",
        "              i_num = distance[cur_row_idx][j - 1] + 1\n",
        "              d_num = distance[prev_row_idx][j] + 1\n",
        "              distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "  return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    if ignore_case == True:\n",
        "        reference = reference\n",
        "        hypothesis = hypothesis\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, remove_space=False):\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    print(f'reference before: {reference}')\n",
        "    print(f'hypothesis before: {hypothesis}')\n",
        "\n",
        "    if isinstance(reference, list):\n",
        "      reference = join_char.join(filter(None,reference[0].values()))\n",
        "    else:\n",
        "        reference = join_char.join(filter(None, reference))\n",
        "\n",
        "    if isinstance(hypothesis, list):\n",
        "      hypothesis = join_char.join(filter(None,hypothesis[0].values()))\n",
        "    else:\n",
        "      reference = join_char.join(filter(None, hypothesis))\n",
        "\n",
        "\n",
        "    edit_distance = levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, delimiter=' '):\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis,\n",
        "                                         delimiter)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, remove_space=False):\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer"
      ],
      "metadata": {
        "id": "WKxghjwnbvQp"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(transcripts):\n",
        "  unique_chars = set(''.join(transcripts))\n",
        "\n",
        "  char_to_id = {char: id + 1 for id, char in enumerate(sorted(unique_chars))}\n",
        "  char_to_id['<pad>'] = 0  # Padding token\n",
        "  char_to_id['<sos>'] = len(char_to_id)  # Start of sequence token\n",
        "  char_to_id['<eos>'] = len(char_to_id)  # End of sequence token\n",
        "  char_to_id['<unk>'] = len(char_to_id)  # Unknown token, for characters not in the vocabulary\n",
        "\n",
        "  return char_to_id\n",
        "\n",
        "vocab = build_vocab(transcribed_audios_df['Transcriptions'])"
      ],
      "metadata": {
        "id": "XcrBjFA8bzRU"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformText:\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "    self.char_map = {}\n",
        "    self.index_map = {}\n",
        "\n",
        "    for ch, idx in vocab.items():\n",
        "      self.char_map[ch] = int(idx)\n",
        "      self.index_map[idx] = ch\n",
        "\n",
        "\n",
        "  # CHANGING TEXT TO INT SEQUENCE\n",
        "  def text_to_int_seq(self, text):\n",
        "    int_sequence = []\n",
        "\n",
        "    for char in text:\n",
        "      char = self.char_map[char]\n",
        "      int_sequence.append(char)\n",
        "    return int_sequence\n",
        "\n",
        "\n",
        "  def int_to_text(self, labels):\n",
        "    string = []\n",
        "\n",
        "    for i in labels:\n",
        "      string.append(self.index_map)\n",
        "    return string\n",
        "\n",
        "\n",
        "text_transform = TransformText(vocab)"
      ],
      "metadata": {
        "id": "XvcvDuQSb3ZB"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, metadata_df, dbx_client):\n",
        "\n",
        "    self.metadata_df = metadata_df\n",
        "    self.dbx_client = dbx_client\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.metadata_df)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.metadata_df.iloc[idx]\n",
        "\n",
        "    audio_path = '/Akan/selected transcribed audios/audios/' + item['Full Filename'].lstrip('/')\n",
        "\n",
        "    # STREAM AUDIO\n",
        "    _, audio_res = self.dbx_client.files_download(path= audio_path)\n",
        "    signal, sample_rate = torchaudio.load(io.BytesIO(audio_res.content))\n",
        "\n",
        "    # CONVERTING STEREO AUDIO TO MONO\n",
        "    if signal.shape[0] == 2:\n",
        "      signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "\n",
        "    transcript = item['Transcriptions']\n",
        "\n",
        "\n",
        "    return signal, transcript"
      ],
      "metadata": {
        "id": "hcyE5h-Jqmat"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_sequence(text, vocabularies):\n",
        "  return [vocabularies.get(char, vocabularies['<unk>']) for char in text]"
      ],
      "metadata": {
        "id": "ik6hgRLWcAjz"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "akan_dataset = MyDataset(transcribed_audios_df[:1000], dbx)"
      ],
      "metadata": {
        "id": "0lLwJHWYrjmC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processing(data):\n",
        "  sample_rate = 44100\n",
        "  mfccs = []\n",
        "  labels = []\n",
        "  input_lengths = []\n",
        "  label_lengths = []\n",
        "\n",
        "  #FEATURE EXTRACTION\n",
        "  mcfcc_transformer = T.MFCC(sample_rate=sample_rate, n_mfcc=13,\n",
        "  melkwargs={\n",
        "      \"n_fft\": 2048,\n",
        "      \"hop_length\": 512,\n",
        "      \"n_mels\": 40,\n",
        "  })\n",
        "\n",
        "  for (signal, transcript) in data:\n",
        "    mfcc = mcfcc_transformer(signal).squeeze(0).transpose(0, 1)\n",
        "    # NORMALIZE WAVEFORM\n",
        "    mfcc = (mfcc - mfcc.mean()) / mfcc.std()\n",
        "    mfccs.append(mfcc)\n",
        "    label = torch.Tensor(text_transform.text_to_int_seq(transcript))\n",
        "    labels.append(label)\n",
        "    input_lengths.append(mfcc.shape[0]//2)\n",
        "    label_lengths.append(len(label))\n",
        "\n",
        "  mfccs = pad_sequence(mfccs, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "  labels = pad_sequence(labels, batch_first=True, padding_value=0.0)\n",
        "\n",
        "  return mfccs, labels, input_lengths, label_lengths"
      ],
      "metadata": {
        "id": "XROBn6qHoitU"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = [akan_dataset.__getitem__(4+i) for i in range(4)]"
      ],
      "metadata": {
        "id": "NibVvkvAIc_k"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Splitting Dataset into Train, Test and Validation Set**"
      ],
      "metadata": {
        "id": "rOGE7bmEcEmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIsoVsQDAcEc",
        "outputId": "473376be-138c-44f8-8036-a7d9e8b8ab8c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a188430a930>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(akan_dataset)\n",
        "train_size = int(0.5*dataset_size)\n",
        "test_size = int((dataset_size - train_size)*0.5)\n",
        "val_size = int(dataset_size - train_size - test_size)\n",
        "\n",
        "train_ds, test_ds, val_ds = random_split(akan_dataset, [train_size, test_size, val_size])"
      ],
      "metadata": {
        "id": "5dA5-qVxcZ78"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODES THE OUTPUT OF THE PREDICTION\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tprint(arg_maxes)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ],
      "metadata": {
        "id": "cUeN5VKScD6O"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Building**"
      ],
      "metadata": {
        "id": "EyzV5BKDc_ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deep Search 2 Architecture**"
      ],
      "metadata": {
        "id": "EEqIScxjc77d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x (batch, channel, feature, time)\n",
        "      x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "      x = self.layer_norm(x)\n",
        "      return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)"
      ],
      "metadata": {
        "id": "YNJGR_z0dbsX"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel,\n",
        "                              stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel,\n",
        "                              stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)"
      ],
      "metadata": {
        "id": "2xQhsAvnc6vi"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2 + 1\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2) # cnn for extracting hierarchical features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.cnn(x)\n",
        "      x = self.rescnn_layers(x)\n",
        "      sizes = x.size()\n",
        "      x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "      x = x.transpose(1, 2) # (batch, time, feature)\n",
        "      x = self.fully_connected(x)\n",
        "      x = self.birnn_layers(x)\n",
        "      x = self.classifier(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "7veac0nfbkoA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "\n",
        "      mfcc, labels, input_lengths, label_lengths = _data\n",
        "      mfcc, labels = mfcc.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(mfcc)  # (batch, time, n_class)\n",
        "      output = F.log_softmax(output, dim=2)\n",
        "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "\n",
        "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "      loss.backward()\n",
        "\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      iter_meter.step()\n",
        "      if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(mfcc), data_len,\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
        "    print('\\nevaluating…')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "      for i, _data in enumerate(test_loader):\n",
        "        mfcc, labels, input_lengths, label_lengths = _data\n",
        "        mfcc, labels = mfcc.to(device), labels.to(device)\n",
        "\n",
        "        output = model(mfcc)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "        for j in range(len(decoded_preds)):\n",
        "            test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "            test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
      ],
      "metadata": {
        "id": "BIl_lxModpiS"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(learning_rate, batch_size, epochs, train_ds, valid_ds):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 94,\n",
        "        \"n_feats\": 13,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Creating DataLoaders for the Train, Test and Validation Datasets\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = DataLoader(dataset=train_ds,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x),\n",
        "                                **kwargs)\n",
        "    test_loader = DataLoader(dataset=test_ds,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x),\n",
        "                                **kwargs)\n",
        "\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter)"
      ],
      "metadata": {
        "id": "2lWQIJ8T9WQr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 5\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "ZuQdorl2pdJi"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(learning_rate, batch_size, epochs, train_ds, test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQD4bU0pd3Ep",
        "outputId": "33c69e84-460d-4814-f551-054de1520895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/500 (0%)]\tLoss: 20.156981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnkQMWSoI15E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHLVUWhyI1bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGnFTaW9feb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4iePkVlv_ab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}