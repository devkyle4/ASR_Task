{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devkyle4/ASR_Task/blob/main/UG_asr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZSfPw6tPnTp",
        "outputId": "ece6d971-20a7-4d4b-9a05-1b52b57de290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dropbox in /usr/local/lib/python3.10/dist-packages (11.36.2)\n",
            "Requirement already satisfied: requests>=2.16.2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (2.31.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from dropbox) (1.16.0)\n",
            "Requirement already satisfied: stone>=2 in /usr/local/lib/python3.10/dist-packages (from dropbox) (3.3.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.16.2->dropbox) (2024.2.2)\n",
            "Requirement already satisfied: ply>=3.4 in /usr/local/lib/python3.10/dist-packages (from stone>=2->dropbox) (3.11)\n"
          ]
        }
      ],
      "source": [
        "!pip install dropbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JOOu2lPEpclq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "import torch\n",
        "import dropbox\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchaudio.transforms as T\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split"
      ],
      "metadata": {
        "id": "lY9A9eK3gfl4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uDTGCd0_A4kv"
      },
      "outputs": [],
      "source": [
        "!pip freeze > requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GMNswJeBagWL"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "dbx_token = userdata.get('dbx_token')\n",
        "dbx = dropbox.Dropbox(dbx_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YrskzXRifD3G"
      },
      "outputs": [],
      "source": [
        "# STREAM DATA FROM DROPBOX\n",
        "\n",
        "def stream_data_from_dropbox(path):\n",
        "  _, res = dbx.files_download(path)\n",
        "  data = io.BytesIO(res.content)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "SGcyb6oviA_q"
      },
      "outputs": [],
      "source": [
        "audio_transcript_path=\"/Akan/selected transcribed audios/selected transcribed audios.xlsx\"\n",
        "audio_transcript_data = stream_data_from_dropbox(audio_transcript_path)\n",
        "transcribed_audios_df = pd.read_excel(audio_transcript_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "  return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "def levenshtein_distance(ref, hyp):\n",
        "  m = len(ref)\n",
        "  n = len(hyp)\n",
        "\n",
        "  # Special Case\n",
        "  if ref == hyp:\n",
        "    return 0\n",
        "  if m == 0:\n",
        "    return n\n",
        "  if n == 0:\n",
        "    return m\n",
        "\n",
        "  if m<n:\n",
        "    ref, hyp = hyp, ref\n",
        "    m, n = n, m\n",
        "\n",
        "\n",
        "  distance = np.zeros((2, n+1), dtype=np.int32)\n",
        "\n",
        "  # initialize distance matrix\n",
        "  for j in range(0,n + 1):\n",
        "      distance[0][j] = j\n",
        "\n",
        "  # calculate levenshtein distance\n",
        "  for i in range(1, m + 1):\n",
        "      prev_row_idx = (i - 1) % 2\n",
        "      cur_row_idx = i % 2\n",
        "      distance[cur_row_idx][0] = i\n",
        "      for j in range(1, n + 1):\n",
        "          if ref[i - 1] == hyp[j - 1]:\n",
        "              distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "          else:\n",
        "              s_num = distance[prev_row_idx][j - 1] + 1\n",
        "              i_num = distance[cur_row_idx][j - 1] + 1\n",
        "              d_num = distance[prev_row_idx][j] + 1\n",
        "              distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "  return distance[m % 2][n]\n",
        "\n",
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    if ignore_case == True:\n",
        "        reference = reference\n",
        "        hypothesis = hypothesis\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)\n",
        "\n",
        "\n",
        "def char_errors(reference, hypothesis, remove_space=False):\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "\n",
        "    if isinstance(reference, list):\n",
        "      reference = join_char.join(filter(None,reference[0].values()))\n",
        "    else:\n",
        "        reference = join_char.join(filter(None, reference))\n",
        "\n",
        "    if isinstance(hypothesis, list):\n",
        "      hypothesis = join_char.join(filter(None,hypothesis[0].values()))\n",
        "    else:\n",
        "      hypothesis = join_char.join(filter(None, hypothesis))\n",
        "\n",
        "    edit_distance = levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)\n",
        "\n",
        "\n",
        "def wer(reference, hypothesis, delimiter=' '):\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis,\n",
        "                                         delimiter)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer\n",
        "\n",
        "\n",
        "def cer(reference, hypothesis, remove_space=False):\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer"
      ],
      "metadata": {
        "id": "WKxghjwnbvQp"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def text_to_sequence(text, vocabularies):\n",
        "#   return [vocabularies.get(char, vocabularies['<unk>']) for char in text]"
      ],
      "metadata": {
        "id": "ik6hgRLWcAjz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(transcripts):\n",
        "  unique_chars = set(''.join(transcripts))\n",
        "\n",
        "  char_to_id = {char: id + 1 for id, char in enumerate(sorted(unique_chars))}\n",
        "  char_to_id['<pad>'] = 0  # Padding token\n",
        "  char_to_id['<sos>'] = len(char_to_id)  # Start of sequence token\n",
        "  char_to_id['<eos>'] = len(char_to_id)  # End of sequence token\n",
        "  char_to_id['<unk>'] = len(char_to_id)  # Unknown token, for characters not in the vocabulary\n",
        "\n",
        "  return char_to_id\n",
        "\n",
        "vocab = build_vocab(transcribed_audios_df['Transcriptions'])"
      ],
      "metadata": {
        "id": "XcrBjFA8bzRU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformText:\n",
        "  def __init__(self, vocab):\n",
        "    self.vocab = vocab\n",
        "    self.char_map = {}\n",
        "    self.index_map = {}\n",
        "\n",
        "    for ch, idx in vocab.items():\n",
        "      self.char_map[ch] = int(idx)\n",
        "      self.index_map[idx] = ch\n",
        "\n",
        "\n",
        "  # CHANGING TEXT TO INT SEQUENCE\n",
        "  def text_to_int_seq(self, text):\n",
        "    int_sequence = []\n",
        "\n",
        "    for char in text:\n",
        "      char = self.char_map[char]\n",
        "      int_sequence.append(char)\n",
        "    return int_sequence\n",
        "\n",
        "\n",
        "  def int_to_text(self, labels):\n",
        "    string = []\n",
        "\n",
        "    for i in labels:\n",
        "      string.append(self.index_map)\n",
        "    return string\n",
        "\n",
        "\n",
        "text_transform = TransformText(vocab)"
      ],
      "metadata": {
        "id": "XvcvDuQSb3ZB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self, metadata_df, dbx_client):\n",
        "\n",
        "    self.metadata_df = metadata_df\n",
        "    self.dbx_client = dbx_client\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.metadata_df)\n",
        "\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.metadata_df.iloc[idx]\n",
        "\n",
        "    audio_path = '/Akan/selected transcribed audios/audios/' + item['Full Filename'].lstrip('/')\n",
        "\n",
        "    # STREAM AUDIO\n",
        "    _, audio_res = self.dbx_client.files_download(path= audio_path)\n",
        "    signal, sample_rate = torchaudio.load(io.BytesIO(audio_res.content))\n",
        "\n",
        "    # CONVERTING STEREO AUDIO TO MONO\n",
        "    if signal.shape[0] == 2:\n",
        "      signal = torch.mean(signal, dim=0, keepdim=True)\n",
        "\n",
        "    transcript = item['Transcriptions']\n",
        "\n",
        "\n",
        "    return signal, transcript"
      ],
      "metadata": {
        "id": "hcyE5h-Jqmat"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_processing(data):\n",
        "  sample_rate = 44100\n",
        "  mfccs = []\n",
        "  labels = []\n",
        "  input_lengths = []\n",
        "  label_lengths = []\n",
        "\n",
        "  #FEATURE EXTRACTION\n",
        "  mcfcc_transformer = T.MFCC(sample_rate=sample_rate, n_mfcc=13,\n",
        "  melkwargs={\n",
        "      \"n_fft\": 2048,\n",
        "      \"hop_length\": 512,\n",
        "      \"n_mels\": 40,\n",
        "  })\n",
        "\n",
        "  for (signal, transcript) in data:\n",
        "    mfcc = mcfcc_transformer(signal).squeeze(0).transpose(0, 1)\n",
        "    # NORMALIZE WAVEFORM\n",
        "    mfcc = (mfcc - mfcc.mean()) / mfcc.std()\n",
        "    mfccs.append(mfcc)\n",
        "    label = torch.Tensor(text_transform.text_to_int_seq(transcript))\n",
        "    labels.append(label)\n",
        "    input_lengths.append(mfcc.shape[0]//2)\n",
        "    label_lengths.append(len(label))\n",
        "\n",
        "  mfccs = pad_sequence(mfccs, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "  labels = pad_sequence(labels, batch_first=True, padding_value=0.0)\n",
        "\n",
        "  return mfccs, labels, input_lengths, label_lengths"
      ],
      "metadata": {
        "id": "XROBn6qHoitU"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#USING ONLY 1000 OF THE DATASET BECAUSE STREAMING ALL THE DATA IS INEFFICIENT\n",
        "akan_dataset = MyDataset(transcribed_audios_df[:1000], dbx)\n",
        "# akan_dataset = MyDataset(transcribed_audios_df[:500], dbx)"
      ],
      "metadata": {
        "id": "0lLwJHWYrjmC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# USING ONLY 4 INSTANCES OF THE DATASET FOR TESTING PURPOSES\n",
        "ds = [akan_dataset.__getitem__(4+i) for i in range(4)]"
      ],
      "metadata": {
        "id": "NibVvkvAIc_k"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Splitting Dataset into Train, Test and Validation Set**"
      ],
      "metadata": {
        "id": "rOGE7bmEcEmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_seed = 42\n",
        "torch.manual_seed(random_seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIsoVsQDAcEc",
        "outputId": "ba32d0ee-904e-40eb-b0c8-1ade3dffe075"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f06f0522970>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_size = len(akan_dataset)\n",
        "train_size = int(0.5*dataset_size)\n",
        "test_size = int((dataset_size - train_size)*0.5)\n",
        "val_size = int(dataset_size - train_size - test_size)\n",
        "\n",
        "train_ds, test_ds, val_ds = random_split(akan_dataset, [train_size, test_size, val_size])"
      ],
      "metadata": {
        "id": "5dA5-qVxcZ78"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECODES THE OUTPUT OF THE PREDICTION\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=92, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\t# print(f'arg_maxes Shape: {arg_maxes}')\n",
        "\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\t# print(f'i: {i} & args: {args}')\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\t# print(f'j: {j} & index: {index}')\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\t\t\tprint(f'decode: {decode}')\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ],
      "metadata": {
        "id": "cUeN5VKScD6O"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Building**"
      ],
      "metadata": {
        "id": "EyzV5BKDc_ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Deep 2 Search Architecture**"
      ],
      "metadata": {
        "id": "EEqIScxjc77d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "      # x (batch, channel, feature, time)\n",
        "      x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "      x = self.layer_norm(x)\n",
        "      return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)"
      ],
      "metadata": {
        "id": "YNJGR_z0dbsX"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel,\n",
        "                              stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel,\n",
        "                              stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)"
      ],
      "metadata": {
        "id": "2xQhsAvnc6vi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalGRU(nn.Module):\n",
        "\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2 + 1\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2) # cnn for extracting hierarchical features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.cnn(x)\n",
        "      x = self.rescnn_layers(x)\n",
        "      sizes = x.size()\n",
        "      x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "      x = x.transpose(1, 2) # (batch, time, feature)\n",
        "      x = self.fully_connected(x)\n",
        "      x = self.birnn_layers(x)\n",
        "      x = self.classifier(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "7veac0nfbkoA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "\n",
        "      mfcc, labels, input_lengths, label_lengths = _data\n",
        "      mfcc, labels = mfcc.to(device), labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      output = model(mfcc)  # (batch, time, n_class)\n",
        "      output = F.log_softmax(output, dim=2)\n",
        "      output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "\n",
        "      loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "      loss.backward()\n",
        "\n",
        "\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "      iter_meter.step()\n",
        "      if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(mfcc), data_len,\n",
        "              100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
        "  try:\n",
        "    print('\\nevaluatingâ€¦')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "      for i, _data in enumerate(test_loader):\n",
        "        mfcc, labels, input_lengths, label_lengths = _data\n",
        "        mfcc, labels = mfcc.to(device), labels.to(device)\n",
        "\n",
        "        output = model(mfcc)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "        decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "\n",
        "        for j in range(len(decoded_preds)):\n",
        "            test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "            test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f'Error processing batch {i}: {e}')\n",
        "\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
      ],
      "metadata": {
        "id": "BIl_lxModpiS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(learning_rate, batch_size, epochs, train_ds, valid_ds):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 93,\n",
        "        \"n_feats\": 13,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    # Creating DataLoaders for the Train, Test and Validation Datasets\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = DataLoader(dataset=train_ds,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x),\n",
        "                                **kwargs)\n",
        "    test_loader = DataLoader(dataset=test_ds,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x),\n",
        "                                **kwargs)\n",
        "\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=92).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'],\n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch, iter_meter)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter)"
      ],
      "metadata": {
        "id": "2lWQIJ8T9WQr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 1\n",
        "epochs = 1"
      ],
      "metadata": {
        "id": "ZuQdorl2pdJi"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(learning_rate, batch_size, epochs, train_ds, test_ds)"
      ],
      "metadata": {
        "id": "gQD4bU0pd3Ep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TnkQMWSoI15E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHLVUWhyI1bV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tGnFTaW9feb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4iePkVlv_ab"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}